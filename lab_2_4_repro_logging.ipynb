{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06b56f6a-8ac2-4006-b160-be0f5dbb1409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create logs folder if it doesn't exist\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Generate timestamped log filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "log_file = f\"logs/run_{timestamp}.log\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),            # Console output\n",
    "        logging.FileHandler(log_file)       # File output\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Example logging\n",
    "logging.info(\"ETL run started\")\n",
    "logging.info(\"Cluster/runtime info: Databricks cluster XYZ\")  # Replace with actual info if available\n",
    "logging.info(\"Configuration values: CSV paths, output paths, etc.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06d9e3c9-d6f6-49f6-97ac-8c709be504ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, random, numpy as np\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "# 1. Fix random seeds\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "logging.info(\"Random seeds fixed\")\n",
    "\n",
    "# 2. Capture environment\n",
    "!pip freeze > requirements.txt\n",
    "logging.info(\"Environment captured in requirements.txt\")\n",
    "\n",
    "# 3. Compute SHA-256 hashes for CSV files\n",
    "def hash_file(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return hashlib.sha256(f.read()).hexdigest()\n",
    "\n",
    "data_hashes = {\n",
    "    \"menu_items.csv\": hash_file(\"data/Restaurant Orders/menu_items.csv\"),\n",
    "    \"order_details.csv\": hash_file(\"data/Restaurant Orders/order_details.csv\")\n",
    "}\n",
    "\n",
    "with open(\"data_hashes.json\", \"w\") as f:\n",
    "    json.dump(data_hashes, f, indent=4)\n",
    "\n",
    "logging.info(\"SHA-256 hashes computed and saved to data_hashes.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c86475f8-88eb-4bb6-9083-b603ebc50805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load CSVs\n",
    "menu = pd.read_csv(\"data/Restaurant Orders/menu_items.csv\")\n",
    "orders = pd.read_csv(\"data/Restaurant Orders/order_details.csv\")\n",
    "logging.info(\"CSV files loaded into DataFrames\")\n",
    "\n",
    "# 2. Basic cleaning\n",
    "menu['category'] = menu['category'].str.strip().str.lower()\n",
    "orders['order_date'] = pd.to_datetime(orders['order_date'])\n",
    "orders['order_time'] = pd.to_datetime(orders['order_time'])\n",
    "logging.info(\"Data cleaned: categories standardized, dates converted\")\n",
    "\n",
    "# 3. Merge tables\n",
    "combined = orders.merge(menu, left_on='item_id', right_on='menu_item_id')\n",
    "logging.info(\"Tables merged on item_id\")\n",
    "\n",
    "# 4. Select useful columns\n",
    "tidy = combined[['order_id', 'order_date', 'order_time', 'item_name', 'category', 'price']]\n",
    "\n",
    "# 5. Compute metrics\n",
    "# Top 5 items by quantity\n",
    "top_items = tidy['item_name'].value_counts().head(5)\n",
    "logging.info(f\"Top 5 items: {top_items.to_dict()}\")\n",
    "\n",
    "# Revenue by category\n",
    "revenue_by_category = tidy.groupby('category')['price'].sum().sort_values(ascending=False)\n",
    "logging.info(f\"Revenue by category: {revenue_by_category.to_dict()}\")\n",
    "\n",
    "# Busiest hour of day\n",
    "tidy['hour'] = tidy['order_time'].dt.hour\n",
    "orders_by_hour = tidy.groupby('hour').size().sort_values(ascending=False)\n",
    "logging.info(f\"Busiest hour(s): {orders_by_hour.to_dict()}\")\n",
    "\n",
    "# 6. Save results\n",
    "output_file = f\"/FileStore/tables/etl_output/metrics_{timestamp}.csv\"\n",
    "\n",
    "logging.info(f\"Metrics saved to {output_file}\")\n",
    "\n",
    "# 7. Assert tests\n",
    "assert not tidy.empty, \"ETL output is empty\"\n",
    "expected_columns = ['order_id', 'order_date', 'order_time', 'item_name', 'category', 'price', 'hour']\n",
    "for col in expected_columns:\n",
    "    assert col in tidy.columns, f\"Missing column: {col}\"\n",
    "logging.info(\"ETL assertions passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29f1c8d-742a-4de5-874c-c81c59ef08d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read CSVs\n",
    "menu_spark = spark.read.csv(\"data/Restaurant Orders/menu_items.csv\", header=True, inferSchema=True)\n",
    "orders_spark = spark.read.csv(\"data/Restaurant Orders/order_details.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Example metric: top item by quantity\n",
    "from pyspark.sql.functions import col, count\n",
    "top_item_spark = orders_spark.groupBy(\"item_id\").agg(count(\"*\").alias(\"quantity\")).orderBy(col(\"quantity\").desc())\n",
    "\n",
    "logging.info(\"PySpark metric computed\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab_2_4_repro_logging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
