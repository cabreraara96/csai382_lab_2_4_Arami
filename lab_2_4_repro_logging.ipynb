{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a1e2395-00ad-4707-9067-1e93cefb55cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab 2.3 — Reproducibility and Logging Basics\n",
    "\n",
    "This notebook demonstrates a reproducible ETL process using Pandas with structured logging, environment capture, and data integrity checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06b56f6a-8ac2-4006-b160-be0f5dbb1409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create logs folder if it doesn't exist\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Generate timestamped log filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "log_file = f\"logs/run_{timestamp}.log\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),            # Console output\n",
    "        logging.FileHandler(log_file)       # File output\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Example logging\n",
    "logging.info(\"ETL run started\")\n",
    "logging.info(\"Cluster/runtime info: Databricks cluster XYZ\")  # Replace with actual info if available\n",
    "logging.info(\"Configuration values: CSV paths, output paths, etc.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5daa5ab0-ad0a-426a-9577-c5d1c759e4d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part B — Logging Setup\n",
    "\n",
    "This section configures Python logging to record ETL execution details.\n",
    "Logs are written both to the console and to a timestamped file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df7b737-0b55-4af4-a052-38cfdcd77c60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create logs folder if it doesn't exist\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Generate timestamped log filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "log_file = f\"logs/run_{timestamp}.log\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(log_file)\n",
    "    ]\n",
    ")\n",
    "\n",
    "logging.info(\"ETL run started\")\n",
    "logging.info(\"Runtime environment: Databricks notebook\")\n",
    "logging.info(\"Input path: data/Restaurant Orders/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f4ba7a-4ef5-401d-9ad2-cefcb39b4df6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part C — Reproducibility Setup\n",
    "\n",
    "This section ensures reproducibility by fixing random seeds, capturing the environment,\n",
    "and hashing input data files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06d9e3c9-d6f6-49f6-97ac-8c709be504ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "# Fix random seeds\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "logging.info(\"Random seeds fixed\")\n",
    "\n",
    "# Capture environment\n",
    "!pip freeze > requirements.txt\n",
    "logging.info(\"Environment saved to requirements.txt\")\n",
    "\n",
    "# Function to hash files\n",
    "def hash_file(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return hashlib.sha256(f.read()).hexdigest()\n",
    "\n",
    "data_hashes = {\n",
    "    \"menu_items.csv\": hash_file(\"data/Restaurant Orders/menu_items.csv\"),\n",
    "    \"order_details.csv\": hash_file(\"data/Restaurant Orders/order_details.csv\")\n",
    "}\n",
    "\n",
    "with open(\"data_hashes.json\", \"w\") as f:\n",
    "    json.dump(data_hashes, f, indent=4)\n",
    "\n",
    "logging.info(\"SHA-256 hashes saved to data_hashes.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0d277c9-db42-4c00-b79c-ed7eb0d09c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part D — ETL with Pandas\n",
    "\n",
    "This section loads, cleans, merges, and analyzes restaurant order data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c86475f8-88eb-4bb6-9083-b603ebc50805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "menu = pd.read_csv(\"data/Restaurant Orders/menu_items.csv\")\n",
    "orders = pd.read_csv(\"data/Restaurant Orders/order_details.csv\")\n",
    "logging.info(\"CSV files loaded\")\n",
    "\n",
    "menu['category'] = menu['category'].str.strip().str.lower()\n",
    "orders['order_date'] = pd.to_datetime(orders['order_date'])\n",
    "orders['order_time'] = pd.to_datetime(orders['order_time'])\n",
    "logging.info(\"Data cleaned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29f1c8d-742a-4de5-874c-c81c59ef08d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined = orders.merge(menu, left_on='item_id', right_on='menu_item_id')\n",
    "logging.info(\"Tables merged\")\n",
    "\n",
    "tidy = combined[['order_id', 'order_date', 'order_time', 'item_name', 'category', 'price']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "088e5429-58fe-40ad-9878-27e813828de8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Top 5 items\n",
    "top_items = tidy['item_name'].value_counts().head(5)\n",
    "logging.info(f\"Top items: {top_items.to_dict()}\")\n",
    "\n",
    "# Revenue by category\n",
    "revenue_by_category = tidy.groupby('category')['price'].sum().sort_values(ascending=False)\n",
    "logging.info(f\"Revenue by category: {revenue_by_category.to_dict()}\")\n",
    "\n",
    "# Busiest hour\n",
    "tidy['hour'] = tidy['order_time'].dt.hour\n",
    "orders_by_hour = tidy.groupby('hour').size().sort_values(ascending=False)\n",
    "logging.info(f\"Busiest hour(s): {orders_by_hour.to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30a07512-90a6-4e01-9197-1832e612c490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Ethics Reflection\n",
    "Sensitive information, such as customer names or payment details, should never be logged because it risks privacy. Similarly, API keys or passwords must not be logged to avoid security breaches. Reproducibility ensures accountability and fairness, allowing results to be verified and trusted when models influence decisions affecting people.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4afbb4b-c2e7-444f-a144-9d84d63577b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define output path\n",
    "output_dir = \"/FileStore/tables/etl_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = f\"{output_dir}/metrics_{timestamp}.csv\"\n",
    "\n",
    "# Save results\n",
    "tidy.to_csv(output_path, index=False)\n",
    "logging.info(f\"Metrics saved to {output_path}\")\n",
    "\n",
    "# Assertions\n",
    "assert not tidy.empty, \"ETL output is empty\"\n",
    "\n",
    "expected_columns = [\n",
    "    'order_id', 'order_date', 'order_time',\n",
    "    'item_name', 'category', 'price', 'hour'\n",
    "]\n",
    "\n",
    "for col in expected_columns:\n",
    "    assert col in tidy.columns, f\"Missing column: {col}\"\n",
    "\n",
    "logging.info(\"ETL assertions passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60708d95-74ee-45f1-b04f-60c7ddbca181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part F — Optional PySpark Comparison\n",
    "\n",
    "This section demonstrates reading the same data using Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce11853d-5fbb-4145-95fc-77025c3dd52a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "menu_spark = spark.read.csv(\"data/Restaurant Orders/menu_items.csv\", header=True, inferSchema=True)\n",
    "orders_spark = spark.read.csv(\"data/Restaurant Orders/order_details.csv\", header=True, inferSchema=True)\n",
    "\n",
    "top_item_spark = (\n",
    "    orders_spark\n",
    "    .groupBy(\"item_id\")\n",
    "    .agg(count(\"*\").alias(\"quantity\"))\n",
    "    .orderBy(col(\"quantity\").desc())\n",
    ")\n",
    "\n",
    "logging.info(\"PySpark metric computed\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab_2_4_repro_logging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
